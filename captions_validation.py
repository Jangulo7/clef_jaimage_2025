# captions_validation.py

# This script is used to validate the captions generated by the model.

# captions_validation.py

import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration
from peft import PeftModel
import pandas as pd
from PIL import Image
from tqdm import tqdm
import os
import json
from bert_score import score as bert_score
from rouge import Rouge

# ---- CONFIGURATION ----
image_dir = "./data/valid/images"
caption_gt_file = "./data/valid/valid_captions.csv"
concepts_file = "validated_concepts.csv"
output_csv = "validated_captions.csv"
metrics_json = "caption_metrics.json"
base_model_path = "llava-hf/llava-hf"
adapter_path = "./models/llava_mistral_concept_lora_jo/checkpoint-20024"
device = "cuda" if torch.cuda.is_available() else "cpu"

batch_size = 4
max_new_tokens = 128
generation_params = {"do_sample": False}
use_concepts_context = True

# ---- LOAD MODEL & PROCESSOR ----
base_model = LlavaForConditionalGeneration.from_pretrained(
    base_model_path,
    device_map="auto",
    torch_dtype=torch.float16
)
processor = AutoProcessor.from_pretrained(base_model_path)
tokenizer = processor.tokenizer
model = PeftModel.from_pretrained(base_model, adapter_path)
model.eval()

# ---- LOAD DATA ----
gt_df = pd.read_csv(caption_gt_file)
image_ids = gt_df["ID"].tolist()
image_to_caption = dict(zip(gt_df["ID"], gt_df["Caption"]))

if use_concepts_context and os.path.exists(concepts_file):
    concepts_df = pd.read_csv(concepts_file)
    image_to_concepts = dict(zip(concepts_df["ID"], concepts_df["CUIs"]))
else:
    image_to_concepts = {}

results = []

# Number of <image> tokens needed for 336x336
N_IMAGE_TOKENS = 576

for idx in tqdm(range(0, len(image_ids), batch_size)):
    batch_ids = image_ids[idx:idx+batch_size]
    images = []
    prompts = []
    real_batch_ids = []
    for image_id in batch_ids:
        img_path_png = os.path.join(image_dir, f"{image_id}.png")
        img_path_jpg = os.path.join(image_dir, f"{image_id}.jpg")
        if os.path.exists(img_path_png):
            img_path = img_path_png
        elif os.path.exists(img_path_jpg):
            img_path = img_path_jpg
        else:
            print(f"Image {image_id} not found as .png or .jpg, skipping.")
            continue

        image = Image.open(img_path).convert("RGB")
        images.append(image)
        real_batch_ids.append(image_id)

        # Build prompt with N_IMAGE_TOKENS <image> tokens
        if use_concepts_context and image_id in image_to_concepts and image_to_concepts[image_id]:
            instruction = (
                f"Known medical concepts in this image: {image_to_concepts[image_id]}\n"
                "As an expert radiologist, provide a detailed descriptive report for this medical image, including all significant findings."
            )
        else:
            instruction = "As an expert radiologist, provide a detailed descriptive report for this medical image, including all significant findings."

        prompt = (
            "USER: " +
            " ".join(["<image>"] * N_IMAGE_TOKENS) +
            "\n" +
            instruction +
            "\nASSISTANT:"
        )
        prompts.append(prompt)

    if not images:
        continue

    # Tokenize prompts, verify <image> tokens
    for p in prompts:
        input_ids = tokenizer(p, return_tensors="pt")["input_ids"][0]
        image_token_id = tokenizer.convert_tokens_to_ids('<image>')
        n_image = (input_ids == image_token_id).sum().item()
        assert n_image == N_IMAGE_TOKENS, f"Prompt must contain exactly {N_IMAGE_TOKENS} <image> tokens, found {n_image}! Prompt: {p}"

    # Prepare batch for LLaVA Next
    inputs = processor(images=images, text=prompts, return_tensors="pt", padding="longest")
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        gen_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
            **generation_params
        )
        input_token_len = inputs['input_ids'].shape[1]
        gen_only = gen_ids[:, input_token_len:]
        generated_captions = processor.batch_decode(gen_only, skip_special_tokens=True)

    for image_id, caption in zip(real_batch_ids, generated_captions):
        results.append({"ID": image_id, "Caption": caption.strip()})

# ---- SAVE CAPTIONS ----
pd.DataFrame(results).to_csv(output_csv, index=False)
print(f"Predictions saved to {output_csv}")

# ---- EVALUATION ----
pred_df = pd.DataFrame(results)
merged = pred_df.merge(gt_df, on="ID", suffixes=("_pred", "_gt"))
references = merged["Caption_gt"].tolist()
candidates = merged["Caption_pred"].tolist()

# BERTScore
P, R, F1 = bert_score(candidates, references, lang="en", rescale_with_baseline=True)
bertscore_metrics = {
    "Precision": float(P.mean()),
    "Recall": float(R.mean()),
    "F1": float(F1.mean())
}
print(f"BERTScore - P: {bertscore_metrics['Precision']:.4f}, R: {bertscore_metrics['Recall']:.4f}, F1: {bertscore_metrics['F1']:.4f}")

# ROUGE
rouge = Rouge()
rouge_scores = rouge.get_scores(candidates, references, avg=True)
print("ROUGE Scores:", rouge_scores)

# ---- SAVE METRICS TO JSON ----
metrics = {
    "BERTScore": bertscore_metrics,
    "ROUGE": rouge_scores
}
with open(metrics_json, "w") as f:
    json.dump(metrics, f, indent=2)
print(f"Metrics saved to {metrics_json}")

